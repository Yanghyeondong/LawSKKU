{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "stemmer = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def contains_korean(text):\n",
    "    # 정규 표현식을 사용하여 문자열에 한국어가 포함되어 있는지 확인합니다.\n",
    "    return bool(re.search(r'[가-힣]', text))\n",
    "\n",
    "def filter_korean_strings(string_list):\n",
    "    # 한국어가 포함된 문자열만 필터링합니다.\n",
    "    return [string for string in string_list if contains_korean(string)]\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords = file.read().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "def remove_strings_with_stopwords(string_list):\n",
    "    filtered_list = []\n",
    "    for string in string_list:\n",
    "        if not any(stopword in string for stopword in stopwords):\n",
    "            filtered_list.append(string)\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_strings_with_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     14\u001b[0m     corpus\u001b[38;5;241m.\u001b[39mappend(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m법내용\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m tokenized_corpus \u001b[38;5;241m=\u001b[39m [remove_strings_with_stopwords(filter_korean_strings(stemmer\u001b[38;5;241m.\u001b[39mmorphs(doc))) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# tokenized_corpus = [doc.split(' ') for doc in corpus]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m bm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_corpus)\n",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     14\u001b[0m     corpus\u001b[38;5;241m.\u001b[39mappend(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m법내용\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m tokenized_corpus \u001b[38;5;241m=\u001b[39m [\u001b[43mremove_strings_with_stopwords\u001b[49m(filter_korean_strings(stemmer\u001b[38;5;241m.\u001b[39mmorphs(doc))) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# tokenized_corpus = [doc.split(' ') for doc in corpus]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m bm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_corpus)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_strings_with_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "\n",
    "# #load stopwords\n",
    "# stopwords = load_stopwords('stopwords.txt')\n",
    "\n",
    "file_path='data/law_data/new_법령이름.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "corpus=[]\n",
    "for i in data:\n",
    "    corpus.append(i['법내용'])\n",
    "tokenized_corpus = [remove_strings_with_stopwords(filter_korean_strings(stemmer.morphs(doc))) for doc in corpus]\n",
    "# tokenized_corpus = [doc.split(' ') for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='data/law_data/final_질문&법령.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data1 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_dic={}\n",
    "for i in data:\n",
    "    law_dic[i['법이름']]=i['법내용']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dic={}\n",
    "for i in data:\n",
    "    answer_dic[i['법내용']]=i['법이름']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict f\n",
    "def predict(question,k):\n",
    "    answer_list=[]\n",
    "    # tokenized_query=question.split(' ')\n",
    "    tokenized_query=stemmer.morphs(question)\n",
    "    tokenized_query=filter_korean_strings(tokenized_query)\n",
    "    tokenized_query=remove_strings_with_stopwords(tokenized_query)\n",
    "    predict_list=bm25.get_top_n(tokenized_query, corpus, n=k)\n",
    "    for pred in predict_list:\n",
    "        answer_list.append(answer_dic[pred])\n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall k \n",
    "def recall(answers, preds):\n",
    "    t=0\n",
    "    for ans in answers:\n",
    "        if ans in preds:\n",
    "            t+=1\n",
    "    \n",
    "    return t/len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test \n",
    "def test(k):\n",
    "    total=0\n",
    "    num=0\n",
    "    for d in tqdm(data1):\n",
    "        if (len(d['법령']>6)):\n",
    "            continue\n",
    "        num+=1\n",
    "        answers=d['법령']\n",
    "        question=d['질문']\n",
    "        preds=predict(question,k)\n",
    "        total+=recall(answers,preds)\n",
    "    return total/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5802/5802 [00:53<00:00, 108.06it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25_result=[]\n",
    "for d in tqdm(data1):\n",
    "    question=d['질문']\n",
    "    preds=predict(question,100)\n",
    "    bm25_result.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "\n",
    "# pkl 파일에서 임베딩 로드\n",
    "with open('embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "# Faiss 인덱스 생성\n",
    "dimension = embeddings.shape[1]# 임베딩의 차원\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 거리 사용\n",
    "\n",
    "# 임베딩을 Faiss 인덱스에 추가\n",
    "index.add(embeddings)\n",
    "\n",
    "#predict \n",
    "def predict_e(question,k,index):\n",
    "    distances, indices = index.search(question, k)\n",
    "    return indices[0]\n",
    "\n",
    "def get_preds_e(predict_number, data):\n",
    "    preds=[]\n",
    "    for i in predict_number:\n",
    "        preds.append(data[i]['법이름'])\n",
    "    return preds\n",
    "\n",
    "def test_e(index,k,data,data1):\n",
    "    with open('q_embeddings.pkl', 'rb') as f:\n",
    "        q_list = pickle.load(f)\n",
    "    total=0\n",
    "    for q_i, q in enumerate(tqdm(q_list)):\n",
    "        predict_number=predict_e(q.reshape(1,-1),k,index)\n",
    "        preds=get_preds_e(predict_number, data)\n",
    "        total+=recall(data1[q_i]['법령'],preds)\n",
    "    return total/len(q_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5802/5802 [00:03<00:00, 1703.39it/s]\n"
     ]
    }
   ],
   "source": [
    "ann_result=[]\n",
    "with open('q_embeddings.pkl', 'rb') as f:\n",
    "    q_list = pickle.load(f)\n",
    "for q_i, q in enumerate(tqdm(q_list)):\n",
    "    predict_number=predict_e(q.reshape(1,-1),100,index)\n",
    "    preds=get_preds_e(predict_number, data)\n",
    "    ann_result.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_result=[]\n",
    "for i in range(len(ann_result)):\n",
    "    mixed_element= bm25_result[i][:50]\n",
    "    for j in range(0,100):\n",
    "        if ann_result[i][j] not in mixed_element:\n",
    "            mixed_element.append(ann_result[i][j])\n",
    "        if (len(mixed_element)==100):\n",
    "            break\n",
    "    mixed_result.append(mixed_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_result=[]\n",
    "for i in range(len(ann_result)):\n",
    "    mixed_element=[]\n",
    "    for j in range(100):\n",
    "        if bm25_result[i][j] not in mixed_element:\n",
    "            mixed_element.append(bm25_result[i][j])\n",
    "        if (len(mixed_element)==100):\n",
    "            break\n",
    "        if ann_result[i][j] not in mixed_element:\n",
    "            mixed_element.append(ann_result[i][j])\n",
    "        if (len(mixed_element)==100):\n",
    "            break        \n",
    "    mixed_result.append(mixed_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19239816408491103\n",
      "0.38793951315465947\n",
      "0.4628964838947627\n",
      "0.5391156462585033\n",
      "0.6656954348004261\n"
     ]
    }
   ],
   "source": [
    "#split(' ')\n",
    "list1=[1, 10, 20, 50,100]\n",
    "for i in list1:\n",
    "    total=0\n",
    "    for j in dev_index: \n",
    "        total+=recall(data1[j]['법령'],mixed_result[j][:i])\n",
    "    print(total/len(dev_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.CrossEncoder import CrossEncoder\n",
    "cross_encoder = CrossEncoder('checkpoints/june/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result =cross_encoder.rank(data1[0]['질문'],data1[0]['ann'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 10, 'score': 0.9555122},\n",
       " {'corpus_id': 21, 'score': 0.9540267},\n",
       " {'corpus_id': 15, 'score': 0.95269436},\n",
       " {'corpus_id': 12, 'score': 0.95072687},\n",
       " {'corpus_id': 20, 'score': 0.94958},\n",
       " {'corpus_id': 19, 'score': 0.9467833},\n",
       " {'corpus_id': 6, 'score': 0.94309556},\n",
       " {'corpus_id': 7, 'score': 0.93889785},\n",
       " {'corpus_id': 16, 'score': 0.9381886},\n",
       " {'corpus_id': 4, 'score': 0.9355388},\n",
       " {'corpus_id': 23, 'score': 0.9339826},\n",
       " {'corpus_id': 31, 'score': 0.9336204},\n",
       " {'corpus_id': 13, 'score': 0.932263},\n",
       " {'corpus_id': 8, 'score': 0.9317442},\n",
       " {'corpus_id': 0, 'score': 0.9313066},\n",
       " {'corpus_id': 3, 'score': 0.93096787},\n",
       " {'corpus_id': 11, 'score': 0.929164},\n",
       " {'corpus_id': 9, 'score': 0.929021},\n",
       " {'corpus_id': 5, 'score': 0.9287259},\n",
       " {'corpus_id': 2, 'score': 0.9284755},\n",
       " {'corpus_id': 1, 'score': 0.92781055},\n",
       " {'corpus_id': 26, 'score': 0.92704934},\n",
       " {'corpus_id': 70, 'score': 0.9251868},\n",
       " {'corpus_id': 41, 'score': 0.9235862},\n",
       " {'corpus_id': 17, 'score': 0.9213349},\n",
       " {'corpus_id': 34, 'score': 0.919388},\n",
       " {'corpus_id': 43, 'score': 0.91828644},\n",
       " {'corpus_id': 32, 'score': 0.91730666},\n",
       " {'corpus_id': 25, 'score': 0.9164271},\n",
       " {'corpus_id': 35, 'score': 0.9136442},\n",
       " {'corpus_id': 36, 'score': 0.9100869},\n",
       " {'corpus_id': 74, 'score': 0.9091161},\n",
       " {'corpus_id': 29, 'score': 0.8974765},\n",
       " {'corpus_id': 38, 'score': 0.89649904},\n",
       " {'corpus_id': 57, 'score': 0.8200247},\n",
       " {'corpus_id': 30, 'score': 0.57078195},\n",
       " {'corpus_id': 84, 'score': 0.55502427},\n",
       " {'corpus_id': 52, 'score': 0.54003936},\n",
       " {'corpus_id': 99, 'score': 0.53272843},\n",
       " {'corpus_id': 91, 'score': 0.5213298},\n",
       " {'corpus_id': 62, 'score': 0.4408461},\n",
       " {'corpus_id': 37, 'score': 0.28242907},\n",
       " {'corpus_id': 22, 'score': 0.25310686},\n",
       " {'corpus_id': 86, 'score': 0.18678236},\n",
       " {'corpus_id': 64, 'score': 0.079171225},\n",
       " {'corpus_id': 73, 'score': 0.06881009},\n",
       " {'corpus_id': 50, 'score': 0.05948965},\n",
       " {'corpus_id': 79, 'score': 0.020775871},\n",
       " {'corpus_id': 18, 'score': 0.017571531},\n",
       " {'corpus_id': 77, 'score': 0.017475776},\n",
       " {'corpus_id': 83, 'score': 0.0103326915},\n",
       " {'corpus_id': 93, 'score': 0.008335061},\n",
       " {'corpus_id': 40, 'score': 0.008313479},\n",
       " {'corpus_id': 44, 'score': 0.007981774},\n",
       " {'corpus_id': 55, 'score': 0.007897311},\n",
       " {'corpus_id': 92, 'score': 0.0075463764},\n",
       " {'corpus_id': 78, 'score': 0.0074484223},\n",
       " {'corpus_id': 87, 'score': 0.007332417},\n",
       " {'corpus_id': 14, 'score': 0.007229214},\n",
       " {'corpus_id': 85, 'score': 0.0071863304},\n",
       " {'corpus_id': 60, 'score': 0.00667193},\n",
       " {'corpus_id': 27, 'score': 0.0060894303},\n",
       " {'corpus_id': 39, 'score': 0.0044385646},\n",
       " {'corpus_id': 28, 'score': 0.0043275678},\n",
       " {'corpus_id': 75, 'score': 0.0025981932},\n",
       " {'corpus_id': 49, 'score': 0.0020472002},\n",
       " {'corpus_id': 65, 'score': 0.0016914891},\n",
       " {'corpus_id': 56, 'score': 0.0014846672},\n",
       " {'corpus_id': 81, 'score': 0.0014471805},\n",
       " {'corpus_id': 63, 'score': 0.001411342},\n",
       " {'corpus_id': 89, 'score': 0.0013918484},\n",
       " {'corpus_id': 67, 'score': 0.0013792863},\n",
       " {'corpus_id': 76, 'score': 0.0013722706},\n",
       " {'corpus_id': 59, 'score': 0.0012821595},\n",
       " {'corpus_id': 47, 'score': 0.0012480674},\n",
       " {'corpus_id': 80, 'score': 0.0012059428},\n",
       " {'corpus_id': 48, 'score': 0.0012010422},\n",
       " {'corpus_id': 71, 'score': 0.0011852421},\n",
       " {'corpus_id': 97, 'score': 0.0011841491},\n",
       " {'corpus_id': 98, 'score': 0.0011669592},\n",
       " {'corpus_id': 96, 'score': 0.0011643553},\n",
       " {'corpus_id': 24, 'score': 0.0011254401},\n",
       " {'corpus_id': 72, 'score': 0.0011158569},\n",
       " {'corpus_id': 51, 'score': 0.0010977383},\n",
       " {'corpus_id': 54, 'score': 0.0010889206},\n",
       " {'corpus_id': 95, 'score': 0.0010679094},\n",
       " {'corpus_id': 82, 'score': 0.0010190299},\n",
       " {'corpus_id': 88, 'score': 0.0010002833},\n",
       " {'corpus_id': 66, 'score': 0.0009947452},\n",
       " {'corpus_id': 90, 'score': 0.0009945836},\n",
       " {'corpus_id': 61, 'score': 0.0009898674},\n",
       " {'corpus_id': 33, 'score': 0.0009884401},\n",
       " {'corpus_id': 42, 'score': 0.00097090437},\n",
       " {'corpus_id': 68, 'score': 0.0009665194},\n",
       " {'corpus_id': 46, 'score': 0.0009608417},\n",
       " {'corpus_id': 69, 'score': 0.0009554992},\n",
       " {'corpus_id': 94, 'score': 0.0009516136},\n",
       " {'corpus_id': 53, 'score': 0.0009472417},\n",
       " {'corpus_id': 45, 'score': 0.00094523217},\n",
       " {'corpus_id': 58, 'score': 0.0009297108}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "file_path='data/law_data/final_질문&법령.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data1 = json.load(file)\n",
    "    \n",
    "\n",
    "def split_list(data, chunk_size=10):\n",
    "    \"\"\"리스트를 chunk_size 단위로 나누어 이중 리스트로 반환합니다.\"\"\"\n",
    "    return [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "for d in data1:\n",
    "    random_integers = random.sample(range(1464), 150)\n",
    "    r_candidate=[]\n",
    "    for r_i in random_integers:\n",
    "        r_candidate.append(data[r_i]['법이름'])\n",
    "    answer_laws=d['법령']\n",
    "    for a in answer_laws:\n",
    "        if a in r_candidate:\n",
    "            r_candidate.remove(a)\n",
    "    \n",
    "    d['random']= r_candidate[:100]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/tako/june/2024_demo_old/data/law_data/final_질문&법령.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data1, file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
